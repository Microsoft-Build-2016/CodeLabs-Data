<a name="HOLTop"></a>
# Building an Intelligent Application using Cortana Analytics Machine Learning APIs #

---

<a name="Overview"></a>
## Overview ##

Machine learning uses computers to run predictive models that learn from existing data in order to forecast future behaviors, outcomes, and trends.

These forecasts or predictions from machine learning can make apps and devices smarter. Azure Machine Learning APIs are ready-to-use APIs that allow you to harness the power of machine learning without requiring you to be a data scientist.
The Recommendations API built with Microsoft Azure Machine Learning helps your customer discover items in your catalog. Customer activity in your digital store is used to recommend items and to improve conversion in your digital store.

A machine learning model is an abstraction of the question you're trying to answer or the outcome you want to predict. Models are trained and evaluated from existing data.

This module includes building an Azure ML Recommendations model, using the results generated by the model to power recommendations on the e-commerce site. You will learn about recommendations systems, natural language processing and how to leverage face and emotion detection.


<a name="Objectives"></a>
### Objectives ###
In this module, you'll see how to:

- Create an Azure Machine Learning Recommendations model
- Analyze the users’ comments using Text Analytics
- Recognize users' emotions by their faces in an image

<a name="Prerequisites"></a>
### Prerequisites ###

The following is required to complete this module:

- [Visual Studio Community 2015][1] or greater.

[1]: https://www.visualstudio.com/products/visual-studio-community-vs

<a name="Setup"></a>
### Setup ###
In order to run the exercises in this module, you'll need to set up your environment first.

1. Open Windows Explorer and go to the module's **Source** folder.
1. Right-click **Setup.cmd** and select **Run as administrator** to launch the setup process that will configure your environment and install the Visual Studio code snippets for this module.
1. If the User Account Control dialog box is shown, confirm the action to proceed.

> **Note:** Make sure you have checked all the dependencies for this module before running the setup.

<a name="CodeSnippets"></a>
### Using the Code Snippets ###

Throughout the module document, you'll be instructed to insert code blocks. For your convenience, most of this code is provided as Visual Studio Code Snippets, which you can access from within Visual Studio 2015 to avoid having to add it manually.

>**Note**: Each exercise is accompanied by a starting solution located in the **Begin** folder of the exercise that allows you to follow each exercise independently of the others. Please be aware that the code snippets that are added during an exercise are missing from these starting solutions and may not work until you have completed the exercise. Inside the source code for an exercise, you'll also find an **End** folder containing a Visual Studio solution with the code that results from completing the steps in the corresponding exercise. You can use these solutions as guidance if you need additional help as you work through this module.

---

<a name="Exercises"></a>
## Exercises ##
This module includes the following exercises:

1. [Adding Recommendations to your app](#Exercise1)
1. [Analyzing the users’ comments using Text Analytics](#Exercise2)
1. [Recognize users' emotions by the faces in an image](#Exercise3)

Estimated time to complete this module: **60 minutes**

>**Note:** When you first start Visual Studio, you must select one of the predefined settings collections. Each predefined collection is designed to match a particular development style and determines window layouts, editor behavior, IntelliSense code snippets, and dialog box options. The procedures in this module describe the actions necessary to accomplish a given task in Visual Studio when using the **General Development Settings** collection. If you choose a different settings collection for your development environment, there may be differences in the steps that you should take into account.

<a name="Exercise1"></a>
### Exercise 1: Adding Recommendations to your app ###

Microsoft Azure Machine Learning's Recommendations supports 3 common scenarios:
- **Frequently Bought Together (FBT) Recommendations**
	In this scenario the recommendations engine will recommend items that are likely to be purchased together in the same transaction with a particular item.

- **Item to Item Recommendations**
	A common scenario that uses this capability, is "people who visited/clicked this item, also visited/clicked this other item".

- **Customer to Item Recommendations**
	Given a customer's prior activity, it is possible to recommend items that the customer may be interested in.
	For instance, given all movies watched by a customer, it is possible to recommend additional content that may be of interest to the customer.

In this exercise, you'll set up and integrate the [Recommendations API](http://gallery.cortanaanalytics.com/MachineLearningAPI/Recommendations-2) in your website.
You will start building a Recommendations model and then, use the results generated by the model to power recommendations on the e-commerce site.

<a name="Ex1Task1"></a>
#### Task 1 - Signing up for the Recommendations API ####

In this task, you'll sign up for the Recommendations API service, and create a recommendations model.

1. You should sign up for the **Recommendations API** service. Go to the **Cortana Analytics Gallery** at http://gallery.cortanaanalytics.com/ and go to the **Machine Learning APIs** section.

1. Select the **Recommendations API**. Notice the Documentation and Sample links.

1. Click **Sign Up**. This link will take you to the Data Market, where you can sign up for the service.

1. Select a plan. You may select the **free tier for 10,000 transactions/month**. This is a free plan, so it won't require any other information (credit card, etc.) You will need a Microsoft Account to sign up for the service. If you don’t have one, you can create a Microsoft Account at no additional cost.

1. After you sign up for Recommendations, you'll be given an **API Key**. You can always see this key at https://datamarket.azure.com/account. Copy this key, as you'll need it when creating your first model.

	![Azure Marketplace](Images/ex1-task1-datamarket-key.png?raw=true "Azure Marketplace")

	_Azure Marketplace_

<a name="Ex1Task2"></a>
#### Task 2 - Creating a recommendations model ####

As mentioned, a Machine Learning Recommendations API was created using Azure Machine Learning by the data scientists at Microsoft.

Now you, as a developer can take advantage of this API and do not have to worry about the details of how to actually build a model. You can take advantage of one that is already created.

In order to build a model, the engine will need two pieces of information, a catalog file, and a usage file. Usually you would query your transactions database for this information. In this case, we have provided sample data.

You simply create a "Recommendations Model" via the API.  The model requires a few things:
- You need your password and API key in order to authenticate to the API
- You create a "project" or model in the API
- You upload 2 sets of data to that project:
	- Your product catalog
	- Your usage information - what customers have done
- The last step is to train the model.

In this task, you have two alternatives to create the recommendations model. You can either use the sample app we provide, or you can use the Recommendations web UI.

##### Using the Console app #####

1. Open in Visual Studio the **RecommendationsSample.sln** solution located at **Source / Ex1 / Begin / RecommendationsSample** folder.

1. Open the **SampleApp.cs** file. The following tasks are going to be executed:
	- Create a Model
	- Add a catalog file
	- Add a usage file
	- Trigger a build for the Model
	- Get a recommendation based on a pair of items

1. Replace the values for the **accountEmail** and **accountKey** fields with your email and the key from the previous task.

1. Run the solution and take note of the **Model Id** that is shown when the application finishes, you'll need it for the following task.

##### Alternative: using the Recommendations UI #####

You can also set up your model using the [Recommendations UI (Beta)](http://recommendations.azurewebsites.net/) and following three simple steps:

1. In the [Recommendations UI](http://recommendations.azurewebsites.net/) site, provide a name for the model and then click **Add Project**.

1. Add a Catalog file: use the **catalog_small.txt** one within the **Resources** folder located at **Source / Ex1 / Begin / RecommendationsSample**.

1. Add a Usage file: use the **usage_data.txt** one within the **Resources** folder located at **Source / Ex1 / Begin / RecommendationsSample**.

1. Create a build: we created a **Recommendation** build type.

	![Recommendations UI](Images/ex1-task1-recommendations-ui.png?raw=true "Recommendations UI")

	_Recommendations UI_

<a name="Ex1Task3"></a>
#### Task 3 - Updating your website to get recommendations ####

In this task, you'll update the PartsUnlimited website to use the Recommendations API. To learn more, go to: https://azure.microsoft.com/en-us/documentation/articles/machine-learning-recommendation-api-documentation/

1. Open in Visual Studio the **PartsUnlimited.sln** solution located at **Source / Ex1 / Begin / PartsUnlimited** folder.

1. Open the **config.json** file located in the **PartsUnlimited** website.

1. Set the AzureMLRecommendations **AccountEmail**, **AccountKey** and **ModelId** settings with the values that you got from the previous task.

1. Now set the Azure DocumentDB **Endpoint** and **AuthKey** using the _URI_ and _Primary Key_ that you obtained while creating it in the first module. If you do not have it, follow these steps: [Creating the DocumentDB database](../Module1-IntelligentApp#task-1---creating-the-documentdb-database).

1. Open the **AzureMLRecommendationEngine.cs** file located in the **Recommendations** folder.

1. Check the **GetRecommendationsAsync** method. Get recommendations of the active build of type "Recommendation" or "Fbt" based on a list of seeds (input) items. The response includes one entry per recommended item, including the Rating of the recommendation; higher number means higher confidence. This is what the product recommendation HTTP request looks like:

	````
	HTTP GET method
	<rootURI>/ItemRecommend?modelId=%27<modelId>%27&itemIds=%27<itemId>%27&numberOfResults=<int>&includeMetadata=<bool>&apiVersion=%271.0%27
	````

1. Run the application and select any product to see the details and recommended items.

	>**Note**: If you see any CSS issue, stop the app, right-click on PartsUnlimited Dependencies and select **Restore Packages**. Then, run the grunt task **default** in the 'Task Runner Explorer' window.

	![Recommendations for an item](Images/ex1-task2-website.png?raw=true "Recommendations for an item")

	_Recommendations for an item_


<a name="Exercise2"></a>
### Exercise 2: Analyzing the users’ comments using Text Analytics ###

Understanding and analyzing unstructured text is an increasingly popular field and includes a wide spectrum of problems such as sentiment analysis, key phrase extraction, topic modeling/extraction, aspect extraction and more.

Text Analytics API is a suite of text analytics services built with Azure Machine Learning. This service is based on research and engineering that originated in Microsoft Research and which has been battle-tested and improved over the past few years by product teams such as Bing and Office. (Read more about [Text Analytics](https://gallery.cortanaanalytics.com/MachineLearningAPI/Text-Analytics-2))

In this exercise, you'll integrate the Text Analytics API for APIs for sentiment analysis into the PartsUnlimited website in order to analyze the user feedback.

<a name="Ex2Task1"></a>
#### Task 1 - Signing up for the Text Analytics API ####

In this task, you'll go to the Azure datamarket and sign up for the Text Analytics Api service.

1. Sign up for the service here: https://datamarket.azure.com/dataset/amla/text-analytics

1. Click **Sign Up** on the right, accept the terms and complete the registration.

1. Go to **My Account** and take note of your **Primary Account Key**, you'll need it later.

<a name="Ex2Task2"></a>
#### Task 2 - Integrating the Text Analytics API into the website ####

In this task, you'll update the PartsUnlimited website to analyze the user feedback.

1. Open in Visual Studio the **PartsUnlimited.sln** solution located at **Source / Ex2 / Begin** folder. Alternatively, you can continue with the solution from the previous exercise.

>**Note**: If you choose to start fresh with the solution at **Source / Ex2 / Begin**, you'll need to configure DocumentDB in the project's config.json.

1. In the **PartsUnlimited** web project, create a new model class within the **Models** folder and name it **Feedback.cs**; replace the content of the file with the following code snippet.

	(Code Snippet - _MachineLearning - FeedbackModel_)

	````C#
	namespace PartsUnlimited.Models
	{
	    public class Feedback
	    {
	        public string Message { get; set; }
	        public double Score { get; set; }
			public string KeyPhrases {get; set; }
	    }
	}
	````

1. Create a new View within the **Views / Store** folder and name it **Feedback.cshtml**; replace the content of the file with the following code snippet. This view will show a different message based on the comment resulting score after calling the Sentiment method. A value below 0.5 is considered closer to negative, so you'll show a text area to expand on the reason why the user isn't happy.

	(Code Snippet - _MachineLearning - FeedbackView_)

	````C#
	@using PartsUnlimited
	@model PartsUnlimited.Models.Feedback

	@{
	ViewBag.Title = $"Your feedback matters";
	}

	<section>
	<div>
		<h2>Your feedback matters</h2>
		<div class="row">

			@if (@Model.Score < 0.5)
			{
								<div class="col-sm-12">
										<h4 style="color:red">So sorry for your experience. We are constantly working to improve…</h4>
										<div>
												@Html.ActionLink("Keep Shopping", "Index", "Home", null, new { @class = "btn" })
										</div>
								</div>
			}
			else
			{
								<div class="col-sm-12">
										<h4 style="color:green">Thanks for your kind feedback!</h4>
										<div>
												@Html.ActionLink("Keep Shopping", "Index", "Home", null, new { @class = "btn" })
										</div>
								</div>
			}

		</div>
		<br />
		<div class="row">
			<p>
				Sentiment score:
			</p>
			<div class="row">
				<div class="col-md-4">
					<div class="progress">
						@if (Model.Score < 0.5)
						{
							<div class="progress-bar progress-bar-danger" role="progressbar" aria-valuenow="@(Model.Score * 100)" aria-valuemin="0" aria-valuemax="100" style="width: @Math.Round(Model.Score *100)%;">
								@Math.Round(Model.Score * 100)%
							</div>
						}
						else
						{
							<div class="progress-bar progress-bar-success" role="progressbar" aria-valuenow="@(Model.Score * 100)" aria-valuemin="0" aria-valuemax="100" style="width: @Math.Round(Model.Score *100)%;">
								@Math.Round(Model.Score * 100)%
							</div>
						}
					</div>
				</div>
			</div>
			<p>Key Phrases:</p>
			<div class="row">
				<div class="col-md-4">
									<ul>
											@if (Model.KeyPhrases != null)
											{
													@foreach (var phrase in Model.KeyPhrases.ToList())
													{
															<li>@phrase</li>
													}
											}
											else
											{
													<li>No key phrases found</li>
											}
									</ul>
				</div>
			</div>
	   </div>
	</div>
	</section>
	````

1. Open the **Details.cshtml** file located in the **Views / Store** folder and add the following code snippet before the last section. This will add a section on the website that will allow the user to send feedback comments about the item.

	(Code Snippet - _MachineLearning - DetailsView_)
	<!-- mark:7-27 -->
	````C#
                <a href="@Url.Action("AddToCart", "ShoppingCart", new { id = Model.ProductId })" class="btn">Add to Cart</a>
            </div>
        </div>
    </section>

		@using (Html.BeginForm("Feedback", "Store"))
		{
		    <section>
		        <h2>Feedback</h2>
		        <div class="row">
		            <div class="col-sm-12 col-md-6 wide-col-padding no-gutter-xs">
		                <div class="col-xs-12  no-gutter-sm">
		                    <p>We'd love to know your thoughts on this product</p>
		                </div>
		                <div class="form-group col-sm-8 col-md-10 no-gutter-sm">
		                    @Html.TextBox("feedback", null, new { @class = "form-control", placeholder = "Feedback" })                    
		                </div>

		                <div class="col-md-12 no-gutter-sm">
		                    <input id="feedback-button" type="submit" value="Submit Feedback" />
		                </div>

		            </div>
		        </div>
		    </section>
		}

		<section class="section-alt">
    	@await Component.InvokeAsync("Recommendations", Model)
    	</section>
	````

1. Add a new entry within the **config.json** file for the Text Analytics service.

	````
	"TextAnalytics": {
		"AccountKey": "{datamarket Account Key}"
	}
	````

1. Create a new folder within **PartsUnlimited** and name it **TextAnalytics**.

1. Add a new class file **SentimentResult.cs** for the API result into the new folder.

	(Code Snippet - _MachineLearning - SentimentResult_)

	````C#
	namespace PartsUnlimited.TextAnalytics
	{
	    public class SentimentResult
	    {
	        public double Score { get; set; }

	        public string[] KeyPhrases { get; set; }
	    }
	}
	````

1. In the same folder, add a new interface **ITextAnalyticsService**. The following methods are going to be Api calls.

	(Code Snippet - _MachineLearning - ITextAnalyticsService_)

	````C#
	namespace PartsUnlimited.TextAnalytics
	{
	    public interface ITextAnalyticsService
	    {
	        SentimentResult GetSentiment(string inputTextEncoded);

	        SentimentResult GetKeyPhrases(string inputTextEncoded);
	    }
	}
	````

1. Now let's add code for the Text Analytics service calls in a new class **TextAnalyticsService**.
	- The constructor set up the Text Analysis Api using the service base Uri and account key that you set in the config.json file.
	- The **GetSentiment** method is the one that makes the request to the _GetSentiment_ method of the Api with the user comments in order to get the score that indicates if it's a positive or negative feedback.
	- The **GetKeyPhrases** method calls the _GetKeyPhrases_ to extract key phrases, which denote the main talking points in the text.

	(Code Snippet - _MachineLearning - TextAnalyticsService_)

	````C#
	namespace PartsUnlimited.TextAnalytics
	{    
	    using System;
	    using System.Net.Http;
	    using System.Net.Http.Headers;
	    using System.Text;
	    using Newtonsoft.Json;

	    public class TextAnalyticsService : ITextAnalyticsService
	    {
	        private HttpClient httpClient = new HttpClient();
	        private const string ServiceBaseUri = "https://api.datamarket.azure.com/";

	        public TextAnalyticsService(string accountKey)
	        {
	            httpClient.BaseAddress = new Uri(ServiceBaseUri);
	            var creds = "AccountKey:" + accountKey;
	            var authorizationHeader = "Basic " + Convert.ToBase64String(Encoding.ASCII.GetBytes(creds));

	            httpClient.DefaultRequestHeaders.Add("Authorization", authorizationHeader);
	            httpClient.DefaultRequestHeaders.Accept.Add(new MediaTypeWithQualityHeaderValue("application/json"));
	        }

	        public SentimentResult GetSentiment(string inputTextEncoded)
	        {
	            var sentimentRequest = "data.ashx/amla/text-analytics/v1/GetSentiment?Text=" + inputTextEncoded;
	            var responseTask = httpClient.GetAsync(sentimentRequest);
	            responseTask.Wait();

	            var response = responseTask.Result;
	            var contentTask = response.Content.ReadAsStringAsync();
	            contentTask.Wait();

	            var content = contentTask.Result;
	            if (!response.IsSuccessStatusCode)
	            {
	                // TODO handle error response
	                return new SentimentResult { Score = 1 };
	            }

	            return JsonConvert.DeserializeObject<SentimentResult>(content);
	        }

	        public SentimentResult GetKeyPhrases(string inputTextEncoded)
	        {
	            var sentimentRequest = "data.ashx/amla/text-analytics/v1/GetKeyPhrases?Text=" + inputTextEncoded;
	            var responseTask = httpClient.GetAsync(sentimentRequest);
	            responseTask.Wait();

	            var response = responseTask.Result;
	            var contentTask = response.Content.ReadAsStringAsync();
	            contentTask.Wait();

	            var content = contentTask.Result;
	            if (!response.IsSuccessStatusCode)
	            {
	                // TODO handle error response
	                return new SentimentResult { Score = 1 };
	            }

	            return JsonConvert.DeserializeObject<SentimentResult>(content);
	        }
	    }
	}
	````

1. Open the **Startup.cs** file to configure the new service dependency injection by adding the following code along with the _PartsUnlimited.TextAnalytics_ using statement.

	````C#
	services.AddSingleton<ITextAnalyticsService, TextAnalyticsService>(s =>
  {
      string accountKey = Configuration["Keys:TextAnalytics:AccountKey"];
      return new TextAnalyticsService(accountKey);
  });
	````

1. Go to the **StoreController.cs** file located in the **Controllers** folder and add the following variable and constructor update for the Text Analysis Api setup. Resolve the missing _PartsUnlimited.TextAnalytics_ using statement.

	(Code Snippet - _MachineLearning - StoreControllerSetup_)
	<!-- mark:5-13 -->
	````C#
	public class StoreController : Controller
	    {
	        // ...

	        private readonly ITextAnalyticsService textAnalyticsService;

					public StoreController(IPartsUnlimitedContext context, IMemoryCache memoryCache, IProductsRepository productsRepository, ITextAnalyticsService textAnalyticsService)
	        {
	            _db = context;
	            _cache = memoryCache;
	            this.productsRepository = productsRepository;
	            this.textAnalyticsService = textAnalyticsService;
	        }
	        // ...

	````

1. Now add the following method. The **Feedback** action is the one that handles the form submit request that includes the user comments and returns the Feedback view that you previously added after calling the Text Analytics service.

	(Code Snippet - _MachineLearning - Feedback_)

	````C#
	public IActionResult Feedback([FromForm]string feedback)
	{
		// get sentiment
		var sentimentResult = textAnalyticsService.GetSentiment(feedback);
		var phrases = textAnalyticsService.GetKeyPhrases(feedback);
		var score = new Feedback() { Score = sentimentResult.Score, KeyPhrases = phrases.KeyPhrases };
		return View(score);
	}
	````

1. Run the application. Select a product and submit a comment, you can try a positive and then a negative feedback to see different results.

	![New feedback section](Images/ex2-task2-feedback-section.png?raw=true "New feedback section")

	_New feedback section_

	![Positive feedback view](Images/ex2-task2-feedback-positive.png?raw=true "Positive feedback view")

	_Positive feedback view_

	![Not that positive feedback](Images/ex2-task2-feedback-not-positive.png?raw=true "Not that positive feedback")

	_Not that positive feedback_

<a name="Exercise3"></a>
### Exercise 3: Recognize users' emotions by the faces in an image ###

What if you could understand the emotion of your customers when they are using your product?

As part of Microsoft Cognitive Services (previously Project Oxford), Face APIs provide state-of-the-art algorithms to process face images, like face detection with gender and age prediction, recognition, alignment and other application level features. (To learn more about [Face APIs](https://microsoft.com/cognitive-services/en-us/face-api)

Now that you've built your Retail website, you'll develop an app for a Retail Store kiosk and use the [Cognitive Services Emotion API](https://microsoft.com/cognitive-services/en-us/emotion-api) to identify emotions in the wild based on facial expressions that are universal.

In this exercise, you'll create a simple Universal Windows app (UWP) and integrate the Emotion API to recognize the emotions expressed by the faces in an image.

<a name="Ex3Task1"></a>
#### Task 1 - Subscribing to the Emotion API and get a subscription key ####

In this task, you'll subscribe to Emotion API, which is part of the Microsoft Cognitive services. See subscription and key management. Both the primary and secondary key can be used in this tutorial. Make sure to follow best practices for keeping your API key secret and secure.

1. Go to [Microsoft Cognitive Services Subscription](https://microsoft.com/cognitive-services/en-us/subscriptions), sign in, accept the terms, and click the **Request New Keys** button on the right.

1. Make sure the **Emotion API - Free** is checked and click **Subscribe**.

	![Emotion API - Request New Keys](Images/ex3-task1-new-keys.png?raw=true "Emotion API - Request New Keys")

	_Emotion API - Request New Keys_

1. Click **Show** on the **Emotion API - Free** Primary Key and keep it handy, you'll need it later. Both the primary and secondary keys can be used in this module. Make sure to follow best practices for keeping your API key secret and secure.

	![Emotion API Keys](Images/ex3-task1-api-key.png?raw=true "Emotion API Keys")

	_Emotion API Keys_


<a name="Ex3Task2"></a>
#### Task 2 - Creating a Universal Windows app ####

The Emotion API takes an image as an input, and returns the confidence across a set of emotions for each face in the image, as well as bounding box for the face, using the Face API. If a user has already called the Face API, they can submit the face rectangle as an optional input.
The emotions detected are anger, contempt, disgust, fear, happiness, neutral, sadness, and surprise. These emotions are understood to be cross-culturally and universally communicated with particular facial expressions.


1. Start **Visual Studio 2015** and create a new project **File > New Project...**

1. In the installed templates tree, navigate to **Visual C# > Windows > Universal** and select the template **Blank App (Windows Universal)**. Enter the name _MyRetailKioskApp_.

	![New Universal Blank App](Images/new-uwp-blank-app.png?raw=true "New Universal Blank App")

	_New Universal Blank App_

1. Click **OK** to create the project.

1. In the NuGet Package Manager window, search for **Microsoft.ProjectOxford.Emotion** and install it.

	![Microsoft Cognitive Services Emotion API](Images/emotion-api-nuget.png?raw=true "Microsoft Cognitive Services API")

	_Microsoft Cognitive Services API_

1. From the Solution Explorer, select the **Package.appxmanifest** file. In the **Capabilities** tab, check **Webcam**.

	![Webcam capability in manifest](Images/app-manifest.png?raw=true "Webcam capability in manifest")

	_Webcam capability in manifest_

1. Now open the **MainPage.xaml** file.

1. Locate the **\<Grid\>** tag in the XAML section of the designer, and add the following markup.

	````XML
	<Grid.ColumnDefinitions>
	  <ColumnDefinition Width="2*"/>
	  <ColumnDefinition Width="1*"/>
	</Grid.ColumnDefinitions>
	<CaptureElement x:Name="capturePreview" Stretch="Uniform" Grid.Column="0"/>

	<StackPanel Grid.Column="1">
	  <Button x:Name="TakePhoto" VerticalAlignment="Bottom" Content="Take photo"/>
	  <Image x:Name="TakenPhoto" Stretch="Uniform"/>
	  <ListView x:Name="EmotionList" Width="Auto"/>
	</StackPanel>
	````

1. Open the **MainPage.xaml.cs** file, and add the following using statements for the Emotion API and the media capture.

	(Code Snippet - _RetailKioskApp - Usings_)

	````C#
	using System.Threading.Tasks;

	using Microsoft.ProjectOxford.Emotion;
	using Microsoft.ProjectOxford.Emotion.Contract;

	using Windows.Devices.Enumeration;
	using Windows.Media.Capture;
	using Windows.Media.MediaProperties;
	using Windows.Storage;
	using Windows.Storage.Streams;
	using Windows.UI.Xaml.Media.Imaging;
	````

1. Define the class level variables to hold the media capture and set a constant with your Emotion API primary key that you got in the previous task.

	(Code Snippet - _RetailKioskApp - ClassVariables_)

	````C#
	private const string EmotionApiKey = "{Emotion API Primary Key}";
	private MediaCapture mediaCapture;
	private bool isCameraFound;
	````

1. Create a method to initialize the camera preview using the _MediaCapture_ variable and the _CapturePreview_ control for the video preview.

	(Code Snippet - _RetailKioskApp - InitializeMediaCapture_)

	````C#
	private async void InitializeMediaCapture()
	{
		 try
		 {
			  this.mediaCapture = new MediaCapture();
			  var devices = await DeviceInformation.FindAllAsync(DeviceClass.VideoCapture);
						 
			  // Use the front camera if found one 
			  if (devices == null || devices.Count == 0)
			  {
					this.isCameraFound = false;
					return;
			  }
						 
			  MediaCaptureInitializationSettings settings;
			  settings = new MediaCaptureInitializationSettings { VideoDeviceId = devices[0].Id }; // 0 => front, 1 => back 
			  settings.StreamingCaptureMode = StreamingCaptureMode.Video;

			  await this.mediaCapture.InitializeAsync(settings);
			  VideoEncodingProperties resolutionMax = null;
			  int max = 0;
			  var resolutions = this.mediaCapture.VideoDeviceController.GetAvailableMediaStreamProperties(MediaStreamType.Photo);

			  for (var i = 0; i < resolutions.Count; i++)
			  {
					var res = (VideoEncodingProperties)resolutions[i];
					if (res.Width * res.Height > max)
					{
						 max = (int)(res.Width * res.Height);
						 resolutionMax = res;
					}
			  }

			  await this.mediaCapture.VideoDeviceController.SetMediaStreamPropertiesAsync(MediaStreamType.Photo, resolutionMax);
			  this.capturePreview.Source = this.mediaCapture;
			  this.isCameraFound = true;
			  await this.mediaCapture.StartPreviewAsync();
		 }
		 catch (Exception ex)
		 {
			  var dialog = new Windows.UI.Popups.MessageDialog("Error while initializing media capture device: " + ex.Message);
			  await dialog.ShowAsync();
			  GC.Collect();
		 }
	}
	````

1. In the **MainPage** constructor add a call for the **InitializeMediaCapture** function so the Webcam preview starts once the app initializes.

	````C#
	public MainPage()
	{
		this.InitializeComponent();
		this.InitializeMediaCapture();
	}
	````

1. Go to the **MainPage.xaml** design surface and double click on the Take photo button to generate the click method in _MainPage.xaml.cs_

1. Add the following code in the **TakePhoto_Click** method and make the method _async_. This code will take a photo from the video preview, use it to invoke the function that calls the Emotions API and show the emotion results.

	(Code Snippet - _RetailKioskApp - TakePhoto_)

	````C#
	if (!isCameraFound)
	{
		 return;
	}

	try
	{   
		 using (var imageStream = new InMemoryRandomAccessStream())
		 {
			  // capture photo and encode it
			  var encodingProperties = ImageEncodingProperties.CreateJpeg();
			  await this.mediaCapture.CapturePhotoToStreamAsync(encodingProperties, imageStream);
			  await imageStream.FlushAsync();
			  imageStream.Seek(0);

			  // display photo preview
			  var img = new BitmapImage();
			  img.SetSource(imageStream);
			  this.TakenPhoto.Source = img;

			  imageStream.Seek(0);

			  // call emotion API
			  var emotionServiceClient = new EmotionServiceClient(EmotionApiKey);
			  var emotionResult = await emotionServiceClient.RecognizeAsync(imageStream.AsStreamForRead());
			  var emotionText = "Face [{0}]{1} Anger: {2:P2}{1} Contempt: {3:P2}{1} Disgust: {4:P2}{1} Fear: {5:P2}{1} Happiness: {6:P2}{1} Neutral: {7:P2}{1} Sadness: {8:P2}{1} Surprise: {9:P2}{1}";
							  
			  // display emotion results
			  this.EmotionList.Items.Clear();
			  for (int i = 0; i < emotionResult.Length; i++)
			  {
					var scores = emotionResult[i].Scores;
					var textBlock = new TextBlock() { TextWrapping = TextWrapping.WrapWholeWords };
					textBlock.Text = string.Format(emotionText, i, Environment.NewLine, scores.Anger, scores.Contempt, scores.Disgust, scores.Fear, scores.Happiness, scores.Neutral, scores.Sadness, scores.Surprise);
					this.EmotionList.Items.Add(textBlock);
			  }
		 }
	}
	catch (Exception ex)
	{
		 var dialog = new Windows.UI.Popups.MessageDialog("Error while taking photo: " + ex.Message);
		 await dialog.ShowAsync();
		 GC.Collect();
	}
	````

	This code first uses the _MediaCapture_ object to capture a photo an store it in a _InMemoryRandomAccessStream_ object using JPEG encoding, the creates a _BitmapImage_ object to display a preview using the Image control in the UI. 

	The stream is then moved back to the begging so it can be passed to the emotion SDK by calling **RecognizeAsync**. Once the results are obtained, they are formatted as strings in a list of _TextBlock_ controls (one per recognized face).

1. Press **F5** to build and run the app. Click **Take photo** and check the results.

	![Running the UWP app](Images/running-uwp-app.png?raw=true "Running the UWP app")

	_Running the UWP app_

### Microsoft Cognitive Services APIs ###

In addition to the [Emotion API](https://www.microsoft.com/cognitive-services/en-us/emotion-api) you can also build your own solutions with any language or development platform. Some examples of the APIs are:

#### Vision ####
- [Computer Vision API](https://www.projectoxford.ai/vision): Understand images and generate thumbnails
- [Face API](https://www.projectoxford.ai/face): See your users with Face Detection and recognition
- [Video API](https://www.projectoxford.ai/video): Understand and transform your videos

#### Speech ####
- [Speech API](https://www.projectoxford.ai/speech): Communicate with your users with speech recognition and synthesis powered by Bing
- [Speaker Recognition API](https://www.projectoxford.ai/spid): Recognize your users from their voice using our state-of-the-art Speaker Recognition API
- [Custom Recognition Intelligent Service (CRIS)](https://www.projectoxford.ai/cris): Customize both language and acoustic models for better speech recognition tailored to your application

#### Language ####
- [Spell Check API](https://www.projectoxford.ai/spellcheck): Detect and correct common and uncommon spelling errors, via the Bing document index
- [Language Understanding Intelligent Service (LUIS)](https://www.projectoxford.ai/luis): Understand natural language commands tailored to your application
- [Web Language Model API](https://www.projectoxford.ai/weblm): Leverage the power of language models trained on web-scale data


---

<a name="Summary"></a>
## Summary ##

By completing this module you should have:

- Added recommendations to your app using the Recommendations API
- Analyzed the users’ comments using the Text Analytics API
- Recognized users' emotions by the faces in an image using the Face API